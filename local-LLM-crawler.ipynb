{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9048d7",
   "metadata": {},
   "source": [
    "# Crawl4AI with local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux installation (run on terminal)\n",
    "# curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Verify\n",
    "# ollama -v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1898166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feebbb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting crawl4ai\n",
      "  Downloading crawl4ai-0.7.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.3/431.3 KB\u001b[0m \u001b[31m94.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.11.11 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (3.12.15)\n",
      "Collecting aiosqlite~=0.20\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: rich>=13.9.4 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (14.1.0)\n",
      "Requirement already satisfied: click>=8.1.7 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (8.3.0)\n",
      "Collecting patchright>=1.49.0\n",
      "  Downloading patchright-1.55.2-py3-none-manylinux1_x86_64.whl (45.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m63.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:26\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=2.10 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (2.11.10)\n",
      "Collecting litellm>=1.53.1\n",
      "  Downloading litellm-1.79.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m64.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25hCollecting lark>=1.2.2\n",
      "  Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 KB\u001b[0m \u001b[31m58.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting rank-bm25~=0.2\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Collecting lxml~=5.3\n",
      "  Downloading lxml-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m57.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: brotli>=1.1.0 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (0.28.1)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (1.26.4)\n",
      "Collecting alphashape>=1.3.1\n",
      "  Downloading alphashape-1.3.1-py2.py3-none-any.whl (13 kB)\n",
      "Collecting humanize>=4.10.0\n",
      "  Downloading humanize-4.14.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 KB\u001b[0m \u001b[31m49.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:--\u001b[0m\n",
      "\u001b[?25hCollecting chardet>=5.2.0\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 KB\u001b[0m \u001b[31m69.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tf-playwright-stealth>=1.1.0\n",
      "  Downloading tf_playwright_stealth-1.2.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: psutil>=6.1.1 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/x64/cp3 (from crawl4ai) (7.1.0)\n",
      "Collecting nltk>=3.9.1\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting shapely>=2.0.0\n",
      "  Downloading shapely-2.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4~=4.12 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (4.14.2)\n",
      "Requirement already satisfied: pillow>=10.4 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (11.3.0)\n",
      "Collecting cssselect>=1.2.0\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (1.1.1)\n",
      "Collecting fake-useragent>=2.0.3\n",
      "  Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 KB\u001b[0m \u001b[31m51.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests~=2.26 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (2.32.5)\n",
      "Collecting snowballstemmer~=2.2\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m74.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m72.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyOpenSSL>=24.3.0\n",
      "  Downloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m91.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio>=4.0.0 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (4.11.0)\n",
      "Collecting PyYAML>=6.0\n",
      "  Downloading pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.3/770.3 KB\u001b[0m \u001b[31m62.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting playwright>=1.49.0\n",
      "  Downloading playwright-1.55.0-py3-none-manylinux1_x86_64.whl (45.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m82.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:22\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles>=24.1.0\n",
      "  Downloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: xxhash~=3.4 in /home/foxhound/.local/lib/python3.10/site-packages (from crawl4ai) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (0.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (6.6.4)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.21.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/foxhound/.local/lib/python3.10/site-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from aiosqlite~=0.20->crawl4ai) (4.15.0)\n",
      "Collecting trimesh>=3.9.8\n",
      "  Downloading trimesh-4.9.0-py3-none-any.whl (736 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.5/736.5 KB\u001b[0m \u001b[31m66.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /home/foxhound/.local/lib/python3.10/site-packages (from alphashape>=1.3.1->crawl4ai) (1.15.3)\n",
      "Collecting rtree>=0.9.7\n",
      "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 KB\u001b[0m \u001b[31m65.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click-log>=0.3.2\n",
      "  Downloading click_log-0.4.0-py2.py3-none-any.whl (4.3 kB)\n",
      "Requirement already satisfied: networkx>=2.5 in /home/foxhound/.local/lib/python3.10/site-packages (from alphashape>=1.3.1->crawl4ai) (3.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from anyio>=4.0.0->crawl4ai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio>=4.0.0->crawl4ai) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/foxhound/.local/lib/python3.10/site-packages (from anyio>=4.0.0->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/foxhound/.local/lib/python3.10/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.8)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.27.2->crawl4ai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/foxhound/.local/lib/python3.10/site-packages (from httpx>=0.27.2->crawl4ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/foxhound/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/foxhound/.local/lib/python3.10/site-packages (from httpx>=0.27.2->crawl4ai) (4.3.0)\n",
      "Collecting fastuuid>=0.13.0\n",
      "  Downloading fastuuid-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 KB\u001b[0m \u001b[31m69.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m67.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=6.8.0 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from litellm>=1.53.1->crawl4ai) (8.7.0)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m93.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/foxhound/.local/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
      "Collecting tiktoken>=0.7.0\n",
      "  Downloading tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai>=1.99.5 in /home/foxhound/.local/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (2.1.0)\n",
      "Requirement already satisfied: tokenizers in /home/foxhound/.local/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (0.22.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/foxhound/.local/lib/python3.10/site-packages (from nltk>=3.9.1->crawl4ai) (2025.9.18)\n",
      "Requirement already satisfied: joblib in /home/foxhound/.local/lib/python3.10/site-packages (from nltk>=3.9.1->crawl4ai) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /home/foxhound/.local/lib/python3.10/site-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
      "Collecting pyee<14,>=13\n",
      "  Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /home/foxhound/.local/lib/python3.10/site-packages (from patchright>=1.49.0->crawl4ai) (3.2.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/foxhound/.local/lib/python3.10/site-packages (from pydantic>=2.10->crawl4ai) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/foxhound/.local/lib/python3.10/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/foxhound/.local/lib/python3.10/site-packages (from pydantic>=2.10->crawl4ai) (2.33.2)\n",
      "Collecting cryptography<47,>=45.0.7\n",
      "  Downloading cryptography-46.0.3-cp38-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m70.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests~=2.26->crawl4ai) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/foxhound/.local/lib/python3.10/site-packages (from requests~=2.26->crawl4ai) (3.4.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from rich>=13.9.4->crawl4ai) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/foxhound/.local/lib/python3.10/site-packages (from rich>=13.9.4->crawl4ai) (4.0.0)\n",
      "Collecting fake-http-header<0.4.0,>=0.3.5\n",
      "  Downloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
      "Collecting cffi>=2.0.0\n",
      "  Downloading cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 KB\u001b[0m \u001b[31m98.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m99.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hpack<5,>=4.1 in /home/foxhound/.local/lib/python3.10/site-packages (from h2<5,>=3->httpx>=0.27.2->crawl4ai) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/foxhound/.local/lib/python3.10/site-packages (from h2<5,>=3->httpx>=0.27.2->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/foxhound/.local/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.3)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.7/382.7 KB\u001b[0m \u001b[31m71.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/foxhound/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.5->litellm>=1.53.1->crawl4ai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/foxhound/.local/lib/python3.10/site-packages (from openai>=1.99.5->litellm>=1.53.1->crawl4ai) (0.11.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /home/foxhound/.local/lib/python3.10/site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.35.3)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 KB\u001b[0m \u001b[31m127.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m130.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/foxhound/.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/foxhound/.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (1.1.10)\n",
      "Requirement already satisfied: filelock in /home/foxhound/.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (25.0)\n",
      "Installing collected packages: snowballstemmer, trimesh, shapely, rtree, rpds-py, rank-bm25, PyYAML, pyee, pycparser, nltk, lxml, lark, humanize, fastuuid, fake-useragent, fake-http-header, cssselect, click-log, chardet, aiosqlite, aiofiles, tiktoken, referencing, playwright, patchright, cffi, alphashape, tf-playwright-stealth, jsonschema-specifications, cryptography, pyOpenSSL, jsonschema, litellm, crawl4ai\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 6.0.2\n",
      "    Uninstalling lxml-6.0.2:\n",
      "      Successfully uninstalled lxml-6.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ddgs 9.6.0 requires lxml>=6.0.0, but you have lxml 5.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.3 aiofiles-25.1.0 aiosqlite-0.21.0 alphashape-1.3.1 cffi-2.0.0 chardet-5.2.0 click-log-0.4.0 crawl4ai-0.7.6 cryptography-46.0.3 cssselect-1.3.0 fake-http-header-0.3.5 fake-useragent-2.2.0 fastuuid-0.14.0 humanize-4.14.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 lark-1.3.1 litellm-1.79.0 lxml-5.4.0 nltk-3.9.2 patchright-1.55.2 playwright-1.55.0 pyOpenSSL-25.3.0 pycparser-2.23 pyee-13.0.0 rank-bm25-0.2.2 referencing-0.37.0 rpds-py-0.28.0 rtree-1.4.1 shapely-2.1.2 snowballstemmer-2.2.0 tf-playwright-stealth-1.2.0 tiktoken-0.12.0 trimesh-4.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install the package\n",
    "!pip install -U crawl4ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad8c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: typing-extensions in /usr/share/positron/resources/app/extensions/positron-python/python_files/lib/ipykernel/py3 (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "# python -m playwright install --with-deps chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a019f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Running Crawl4AI health check\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mTEST\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ Testing crawling capabilities\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mEXPORT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m.. ℹ Exporting media \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mPDF/MHTML/screenshot\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m took \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m65s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m07s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m02s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m10s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● ✅ Crawling test passed! \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fb3c2",
   "metadata": {},
   "source": [
    "# Basic web crawling\n",
    "\n",
    "Provided in the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc111f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37cd7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                   \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m75s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                   \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                   \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m75s \u001b[0m\n",
      "# Example Domain\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "[Learn more](https://iana.org/domains/example)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "\n",
    "async def main():\n",
    "    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n",
    "    run_conf = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://example.com\",\n",
    "            config=run_conf\n",
    "        )\n",
    "        print(result.markdown)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beab2d",
   "metadata": {},
   "source": [
    "## Use with local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download your model in terminal\n",
    "# ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b3f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-17>:17: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                 \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m86s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                 \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m01s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                 \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m5.\u001b[0m\u001b[32m49s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                 \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m36s \u001b[0m\n",
      "Extracted items: [{'name': 'Countries of the World: A Simple Example', 'price': None, 'title': 'Name'}, {'name': 'Hockey Teams: Forms, Searching and Pagination', 'price': None, 'title': 'Price'}, {'name': 'Oscar Winning Films: AJAX and Javascript', 'price': None, 'title': 'Name'}, {'name': 'Turtles All the Way Down: Frames & iFrames', 'price': None, 'title': 'Price'}, {'name': \"Advanced Topics: Real World Challenges You'll Encounter\", 'price': None, 'title': 'Name'}, {'index': 0, 'error': True, 'tags': ['error'], 'content': ['{\\\\\"properties\\\\\": {\\\\\"name\\\\\": {\\\\\"title\\\\\": \\\\\"Name\\\\\", \\\\\"type\\\\\": \\\\\"string\\\\\"}, \\\\\"price\\\\\": {\\\\\"title\\\\\": \\\\\"Price\\\\\", \\\\\"type\\\\\": \\\\\"string\\\\\"}}, \\\\\"required\\\\\": [\\\\\"name\\\\\", \\\\\"price\\\\\"], \\\\\"title\\\\\": \\\\\"Product\\\\\", \\\\\"type\\\\\": \\\\\"object\\\\\"}']}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               636\n",
      "Prompt                   936\n",
      "Total                  1,572\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   636          936        1,572\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\n",
    "from crawl4ai import LLMExtractionStrategy\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/llama3.2:1b\", api_token=None),\n",
    "        schema=Product.schema_json(), # Or use model_json_schema()\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"Extract all the titles of each article in the website.\",\n",
    "        chunk_token_threshold=1000,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=True,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(headless=True)\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.scrapethissite.com/pages/\",\n",
    "            config=crawl_config\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted items:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
