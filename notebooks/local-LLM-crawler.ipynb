{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9048d7",
   "metadata": {},
   "source": [
    "# Crawl4AI with local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc557c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "run \"../src/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a019f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Running Crawl4AI health check\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mTEST\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ Testing crawling capabilities\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mEXPORT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m.. ℹ Exporting media \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mPDF/MHTML/screenshot\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m took \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m38s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m91s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m02s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m93s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● ✅ Crawling test passed! \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fb3c2",
   "metadata": {},
   "source": [
    "# Basic web crawling\n",
    "\n",
    "Provided in the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc111f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37cd7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                                                     \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m72s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                                                     \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://example.com\u001b[0m\u001b[32m                                                                     \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m72s \u001b[0m\n",
      "# Example Domain\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "[Learn more](https://iana.org/domains/example)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "\n",
    "async def main():\n",
    "    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n",
    "    run_conf = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://example.com\",\n",
    "            config=run_conf\n",
    "        )\n",
    "        print(result.markdown)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beab2d",
   "metadata": {},
   "source": [
    "## Use with local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download your model in terminal\n",
    "# ollama pull qwen2.5:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m12s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m28s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m5.\u001b[0m\u001b[32m41s \u001b[0m\n",
      "Extracted items: [{'title': 'Countries of the World: A Simple Example', 'summary': 'A single page that lists information about all the countries in the world. Good for those just get started with web scraping.'}, {'title': 'Hockey Teams: Forms, Searching and Pagination', 'summary': 'Browse through a database of NHL team stats since 1990. Practice building a scraper that handles common website interface components.'}, {'title': 'Oscar Winning Films: AJAX and Javascript', 'summary': 'Click through a bunch of great films. Learn how content is added to the page asynchronously with Javascript and how you can scrape it.'}, {'title': 'Turtles All the Way Down: Frames & iFrames', 'summary': 'Some older sites might still use frames to break up thier pages. Modern ones might be using iFrames to expose data. Learn about turtles as you scrape content inside frames.'}, {'title': \"Advanced Topics: Real World Challenges You'll Encounter\", 'summary': \"Scraping real websites, you're likely run into a number of common gotchas. Get practice with spoofing headers, handling logins & session cookies, finding CSRF tokens, and other common network errors.\"}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               277\n",
      "Prompt                   961\n",
      "Total                  1,238\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   277          961        1,238\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\n",
    "from crawl4ai import LLMExtractionStrategy\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the titles and the description in JSON format like this:\n",
    "        {\"title\": \"title name\", \"description: \"description text\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=1000,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=True,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 500}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(headless=True)\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.scrapethissite.com/pages/\",\n",
    "            config=crawl_config\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted items:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "        \n",
    "        return data \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f64f6",
   "metadata": {},
   "source": [
    "### \"Real\" website\n",
    "\n",
    "Let's try with a real world website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972026f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the name and the price in JSON format like this:\n",
    "        {\"name\": \"product name\", \"price\": \"price value\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=True,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.microcenter.com/product/670842/intel-core-i7-14700k-raptor-lake-s-refresh-34ghz-twenty-core-lga-1700-boxed-processor-heatsink-not-included\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-heatsink-not-included\u001b[0m\u001b[32m  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m74s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-heatsink-not-included\u001b[0m\u001b[32m  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-heatsink-not-included\u001b[0m\u001b[32m  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m34s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-heatsink-not-included\u001b[0m\u001b[32m  \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m09s \u001b[0m\n",
      "Extracted item: [{'name': 'Intel Core i7-14700K Raptor Lake-S Refresh 34GHz Twenty-Core LGA 1700 Boxed Processor Heatsink Not Included', 'price': '$299.99'}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion                71\n",
      "Prompt                   719\n",
      "Total                    790\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                    71          719          790\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4406ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31369b",
   "metadata": {},
   "source": [
    "## Limitations with complex website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the name and the price in JSON format like this:\n",
    "        {\"name\": \"product name\", \"price\": \"price value\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=True,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJKBV/ref=sr_1_1?sr=8-1\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55c9d0",
   "metadata": {},
   "source": [
    "## Unstructured webstie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\n",
    "from crawl4ai import LLMExtractionStrategy\n",
    "\n",
    "class Product(BaseModel):\n",
    "    summary: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content make a summary including the program description, courses and admision dates\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://extension.harvard.edu/academics/programs/computer-science-masters-degree-program/#program-overview\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acec473",
   "metadata": {},
   "source": [
    "Structured data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f45f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m71s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m07s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m5.\u001b[0m\u001b[32m15s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m94s \u001b[0m\n",
      "Extracted item: [{'program': \"The program offers a Master's degree in Computer Science that can be pursued online, during evenings, or at your own pace. The program is designed for lifelong learners from high school to retirement age.\", 'courses': 'Courses are available on various topics such as Artificial Intelligence, Cybersecurity, Data Science, and Programming. Students have the option to explore these courses in a variety of formats including online, evening classes, and self-paced learning.', 'admission': \"To be admitted into the program, applicants should ideally possess a bachelor's degree from an accredited institution, proficiency in programming languages such as Java, Python, or C++, and some work experience in a technical field. Additionally, they must have excellent problem-solving skills, attention to detail, and critical thinking abilities.\", 'error': False}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               180\n",
      "Prompt                 4,096\n",
      "Total                  4,276\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   180        4,096        4,276\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\n",
    "from crawl4ai import LLMExtractionStrategy\n",
    "\n",
    "class Product(BaseModel):\n",
    "    program: str\n",
    "    courses: str\n",
    "    admission: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the program overview, the courses and the admission sections in JSON format like this:\n",
    "        {\"program\": \"program overview\", \"courses\": \"courses\", \"admission\":\"admission\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://extension.harvard.edu/academics/programs/computer-science-masters-degree-program/#program-overview\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c17e3",
   "metadata": {},
   "source": [
    "Extracted item: [{'program': \"The program offers a Master's degree in Computer Science that can be pursued online, during evenings, or at your own pace. The program is designed for lifelong learners from high school to retirement age.\", 'courses': 'Courses are available on various topics such as Artificial Intelligence, Cybersecurity, Data Science, and Programming. Students have the option to explore these courses in a variety of formats including online, evening classes, and self-paced learning.', 'admission': \"To be admitted into the program, applicants should ideally possess a bachelor's degree from an accredited institution, proficiency in programming languages such as Java, Python, or C++, and some work experience in a technical field. Additionally, they must have excellent problem-solving skills, attention to detail, and critical thinking abilities.\", 'error': False}]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
