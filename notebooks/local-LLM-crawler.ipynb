{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9048d7",
   "metadata": {},
   "source": [
    "# Crawl4AI with local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc557c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%run \"../src/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Running Crawl4AI health check\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mTEST\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ Testing crawling capabilities\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;36m[\u001b[0m\u001b[36mEXPORT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m.. ℹ Exporting media \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mPDF/MHTML/screenshot\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m took \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m38s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m91s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m02s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                                                    \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m93s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● ✅ Crawling test passed! \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# crawl4ai health check\n",
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc111f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fb3c2",
   "metadata": {},
   "source": [
    "## Basic web crawling\n",
    "\n",
    "Example provided in the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m15s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m01s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m16s \u001b[0m\n",
      "  * [ ![](https://www.scrapethissite.com/static/images/scraper-icon.png) Scrape This Site ](https://www.scrapethissite.com/)\n",
      "  * [ ](https://www.scrapethissite.com/pages/)\n",
      "  * [ ](https://www.scrapethissite.com/lessons/)\n",
      "  * [ ](https://www.scrapethissite.com/faq/)\n",
      "  * [ Login ](https://www.scrapethissite.com/login/)\n",
      "\n",
      "\n",
      "# Web Scraping Sandbox\n",
      "* * *\n",
      "###  [Countries of the World: A Simple Example](https://www.scrapethissite.com/pages/simple/)\n",
      "A single page that lists information about all the countries in the world. Good for those just get started with web scraping. \n",
      "* * *\n",
      "###  [Hockey Teams: Forms, Searching and Pagination](https://www.scrapethissite.com/pages/forms/)\n",
      "Browse through a database of NHL team stats since 1990. Practice building a scraper that handles common website interface components. \n",
      "* * *\n",
      "###  [Oscar Winning Films: AJAX and Javascript](https://www.scrapethissite.com/pages/ajax-javascript/)\n",
      "Click through a bunch of great films. Learn how content is added to the page asynchronously with Javascript and how you can scrape it. \n",
      "* * *\n",
      "###  [Turtles All the Way Down: Frames & iFrames](https://www.scrapethissite.com/pages/frames/)\n",
      "Some older sites might still use frames to break up thier pages. Modern ones might be using iFrames to expose data. Learn about turtles as you scrape content inside frames. \n",
      "* * *\n",
      "###  [Advanced Topics: Real World Challenges You'll Encounter](https://www.scrapethissite.com/pages/advanced/)\n",
      "Scraping real websites, you're likely run into a number of common gotchas. Get practice with spoofing headers, handling logins & session cookies, finding CSRF tokens, and other common network errors. \n",
      "* * *\n",
      "Lessons and Videos © Hartley Brody 2023 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    browser_conf = BrowserConfig(headless=True)\n",
    "    run_conf = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.scrapethissite.com/pages/\",\n",
    "            config=run_conf\n",
    "        )\n",
    "        print(result.markdown)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beab2d",
   "metadata": {},
   "source": [
    "## Use with local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download your preferred model in terminal\n",
    "# ollama pull qwen2.5:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m80s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m25s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.scrapethissite.com/pages/\u001b[0m\u001b[32m                              \u001b[0m\n",
      "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m8.\u001b[0m\u001b[32m06s \u001b[0m\n",
      "Extracted items: [{'name': 'Countries of the World: A Simple Example', 'description': 'A single page that lists information about all the countries in the world. Good for those just get started with web scraping.'}, {'name': 'Hockey Teams: Forms, Searching and Pagination', 'description': 'Browse through a database of NHL team stats since 1990. Practice building a scraper that handles common website interface components.'}, {'name': 'Oscar Winning Films: AJAX and Javascript', 'description': 'Click through a bunch of great films. Learn how content is added to the page asynchronously with Javascript and how you can scrape it.'}, {'name': 'Turtles All the Way Down: Frames & iFrames', 'description': 'Some older sites might still use frames to break up thier pages. Modern ones might be using iFrames to expose data. Learn about turtles as you scrape content inside frames.'}, {'name': \"Advanced Topics: Real World Challenges You'll Encounter\", 'description': \"Scraping real websites, you're likely run into a number of common gotchas. Get practice with spoofing headers, handling logins & session cookies, finding CSRF tokens, and other common network errors.\"}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               277\n",
      "Prompt                   986\n",
      "Total                  1,263\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   277          986        1,263\n"
     ]
    }
   ],
   "source": [
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the titles and the description in JSON format like this:\n",
    "        {\"title\": \"title name\", \"description: \"description text\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=1000,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 500}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.scrapethissite.com/pages/\",\n",
    "            config=crawl_config\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted items:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "        \n",
    "        return data \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca77bb",
   "metadata": {},
   "source": [
    "Extracted items: [{'name': 'Countries of the World: A Simple Example', 'description': 'A single page that lists information about all the countries in the world. Good for those just get started with web scraping.'}, {'name': 'Hockey Teams: Forms, Searching and Pagination', 'description': 'Browse through a database of NHL team stats since 1990. Practice building a scraper that handles common website interface components.'}, {'name': 'Oscar Winning Films: AJAX and Javascript', 'description': 'Click through a bunch of great films. Learn how content is added to the page asynchronously with Javascript and how you can scrape it.'}, {'name': 'Turtles All the Way Down: Frames & iFrames', 'description': 'Some older sites might still use frames to break up thier pages. Modern ones might be using iFrames to expose data. Learn about turtles as you scrape content inside frames.'}, {'name': \"Advanced Topics: Real World Challenges You'll Encounter\", 'description': \"Scraping real websites, you're likely run into a number of common gotchas. Get practice with spoofing headers, handling logins & session cookies, finding CSRF tokens, and other common network errors.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f64f6",
   "metadata": {},
   "source": [
    "## Real website\n",
    "\n",
    "Let's try with a real world website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972026f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-he\u001b[0m\n",
      "\u001b[4;32matsink-not-included\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m57s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-he\u001b[0m\n",
      "\u001b[4;32matsink-not-included\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-he\u001b[0m\n",
      "\u001b[4;32matsink-not-included\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m53s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://www.microcenter.com/product/670842/intel...e-lga-1700-boxed-processor-he\u001b[0m\n",
      "\u001b[4;32matsink-not-included\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m11s \u001b[0m\n",
      "Extracted item: [{'name': 'Intel Core i7-14700K Raptor Lake-S Refresh 34GHz Twenty-Core LGA 1700 Boxed Processor Heatsink Not Included', 'price': '$299.99'}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion                81\n",
      "Prompt                   716\n",
      "Total                    797\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                    81          716          797\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the name and the price in JSON format like this:\n",
    "        {\"name\": \"product name\", \"price\": \"price value\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.microcenter.com/product/670842/intel-core-i7-14700k-raptor-lake-s-refresh-34ghz-twenty-core-lga-1700-boxed-processor-heatsink-not-included\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4406ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31369b",
   "metadata": {},
   "source": [
    "## Limitations with complex website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba36aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJK\u001b[0m\n",
      "\u001b[4;32mBV/\u001b[0m\u001b[4;32mref\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32msr_1_1\u001b[0m\u001b[4;32m?\u001b[0m\u001b[4;32msr\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32m8\u001b[0m\u001b[4;32m-1\u001b[0m\u001b[32m | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m86s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJK\u001b[0m\n",
      "\u001b[4;32mBV/\u001b[0m\u001b[4;32mref\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32msr_1_1\u001b[0m\u001b[4;32m?\u001b[0m\u001b[4;32msr\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32m8\u001b[0m\u001b[4;32m-1\u001b[0m\u001b[32m | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;33m0.\u001b[0m\u001b[33m25\u001b[0m\u001b[32ms \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJK\u001b[0m\n",
      "\u001b[4;32mBV/\u001b[0m\u001b[4;32mref\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32msr_1_1\u001b[0m\u001b[4;32m?\u001b[0m\u001b[4;32msr\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32m8\u001b[0m\u001b[4;32m-1\u001b[0m\u001b[32m | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m29s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJK\u001b[0m\n",
      "\u001b[4;32mBV/\u001b[0m\u001b[4;32mref\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32msr_1_1\u001b[0m\u001b[4;32m?\u001b[0m\u001b[4;32msr\u001b[0m\u001b[4;32m=\u001b[0m\u001b[4;32m8\u001b[0m\u001b[4;32m-1\u001b[0m\u001b[32m | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m8.\u001b[0m\u001b[32m41s \u001b[0m\n",
      "Extracted item: [{'index': 0, 'error': True, 'tags': ['error'], 'content': ['{\\n  {\\n    \"name\": \"Bose Cancelling Wireless Bluetooth Headphones\",\\n    \"price\": \"$249.00\"\\n  }\\n}']}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion                46\n",
      "Prompt                 4,096\n",
      "Total                  4,142\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                    46        4,096        4,142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the name and the price in JSON format like this:\n",
    "        {\"name\": \"product name\", \"price\": \"price value\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.amazon.com/Bose-Cancelling-Wireless-Bluetooth-Headphones/dp/B07Q9MJKBV/ref=sr_1_1?sr=8-1\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content is presumably JSON\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d0bc2",
   "metadata": {},
   "source": [
    "Extracted item: [{'index': 0, 'error': True, 'tags': ['error'], 'content': ['{\\n  {\\n    \"name\": \"Bose Cancelling Wireless Bluetooth Headphones\",\\n    \"price\": \"$249.00\"\\n  }\\n}']}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55c9d0",
   "metadata": {},
   "source": [
    "## Where LLM shines\n",
    "\n",
    "Constantly updating websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1cd609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m43s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m05s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m01s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m8.\u001b[0m\u001b[32m50s \u001b[0m\n",
      "Extracted item: [{'summary': 'The Computer Science Master’s Degree Program at Harvard Extension School is an advanced degree program that can be completed in 2-5 years depending on the pace and number of courses taken each semester. The program offers year-round study, allowing students to take courses in fall, January, spring, and summer. Eligible students receive grant funds to cover a portion of tuition costs each term, in addition to federal financial aid options. Students can explore courses at https://courses.dce.harvard.edu/ today.'}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               121\n",
      "Prompt                 4,096\n",
      "Total                  4,217\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   121        4,096        4,217\n"
     ]
    }
   ],
   "source": [
    "class Product(BaseModel):\n",
    "    summary: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content make a summary including the program description, courses and admision dates\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://extension.harvard.edu/academics/programs/computer-science-masters-degree-program/#program-overview\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f7f98",
   "metadata": {},
   "source": [
    "Extracted item: [{'summary': 'The Computer Science Master’s Degree Program at Harvard Extension School is an advanced degree program that can be completed in 2-5 years depending on the pace and number of courses taken each semester. The program offers year-round study, allowing students to take courses in fall, January, spring, and summer. Eligible students receive grant funds to cover a portion of tuition costs each term, in addition to federal financial aid options. Students can explore courses at https://courses.dce.harvard.edu/ today.'}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acec473",
   "metadata": {},
   "source": [
    "Structure the desired information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m6\u001b[0m\u001b[36m \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m71s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m07s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mEXTRACT\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m. ■ \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m5.\u001b[0m\u001b[32m15s \u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\n",
      "\u001b[4;32mhttps://extension.harvard.edu/academics/programs...science-masters-degree-progra\u001b[0m\n",
      "\u001b[4;32mm/#program-overview\u001b[0m\u001b[32m  | \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m94s \u001b[0m\n",
      "Extracted item: [{'program': \"The program offers a Master's degree in Computer Science that can be pursued online, during evenings, or at your own pace. The program is designed for lifelong learners from high school to retirement age.\", 'courses': 'Courses are available on various topics such as Artificial Intelligence, Cybersecurity, Data Science, and Programming. Students have the option to explore these courses in a variety of formats including online, evening classes, and self-paced learning.', 'admission': \"To be admitted into the program, applicants should ideally possess a bachelor's degree from an accredited institution, proficiency in programming languages such as Java, Python, or C++, and some work experience in a technical field. Additionally, they must have excellent problem-solving skills, attention to detail, and critical thinking abilities.\", 'error': False}]\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Type                   Count\n",
      "------------------------------\n",
      "Completion               180\n",
      "Prompt                 4,096\n",
      "Total                  4,276\n",
      "\n",
      "=== Usage History ===\n",
      "Request #    Completion       Prompt        Total\n",
      "------------------------------------------------\n",
      "1                   180        4,096        4,276\n"
     ]
    }
   ],
   "source": [
    "class Product(BaseModel):\n",
    "    program: str\n",
    "    courses: str\n",
    "    admission: str\n",
    "\n",
    "async def main():\n",
    "    # 1. Define the LLM extraction strategy\n",
    "    llm_strategy = LLMExtractionStrategy(\n",
    "        llm_config = LLMConfig(provider=\"ollama/qwen2.5:3b\", api_token=None),\n",
    "        schema=Product.model_json_schema(),\n",
    "        extraction_type=\"schema\",\n",
    "        instruction=\"\"\" \n",
    "        From the crawled content\n",
    "        extract the program overview, the courses and the admission sections in JSON format like this:\n",
    "        {\"program\": \"program overview\", \"courses\": \"courses\", \"admission\":\"admission\"}\n",
    "        \"\"\",\n",
    "        chunk_token_threshold=500,\n",
    "        overlap_rate=0.0,\n",
    "        apply_chunking=False,\n",
    "        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n",
    "        extra_args={\"temperature\": 0.0, \"max_tokens\": 1000}\n",
    "    )\n",
    "\n",
    "    # 2. Build the crawler config\n",
    "    crawl_config = CrawlerRunConfig(\n",
    "        extraction_strategy=llm_strategy,\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    # 3. Create a browser config if needed\n",
    "    browser_cfg = BrowserConfig(\n",
    "        headless=True,\n",
    "        text_mode=True,\n",
    "        light_mode=True\n",
    "        )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        # 4. Let's say we want to crawl a single page\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://extension.harvard.edu/academics/programs/computer-science-masters-degree-program/#program-overview\",\n",
    "            config=crawl_config\n",
    "            \n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # 5. The extracted content\n",
    "            data = json.loads(result.extracted_content)\n",
    "            print(\"Extracted item:\", data)\n",
    "\n",
    "            # 6. Show usage stats\n",
    "            llm_strategy.show_usage()  # prints token usage\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c17e3",
   "metadata": {},
   "source": [
    "Extracted item: [{'program': \"The program offers a Master's degree in Computer Science that can be pursued online, during evenings, or at your own pace. The program is designed for lifelong learners from high school to retirement age.\", 'courses': 'Courses are available on various topics such as Artificial Intelligence, Cybersecurity, Data Science, and Programming. Students have the option to explore these courses in a variety of formats including online, evening classes, and self-paced learning.', 'admission': \"To be admitted into the program, applicants should ideally possess a bachelor's degree from an accredited institution, proficiency in programming languages such as Java, Python, or C++, and some work experience in a technical field. Additionally, they must have excellent problem-solving skills, attention to detail, and critical thinking abilities.\", 'error': False}]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
